{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c3ab84-5f87-4001-ae51-0affb77f206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "import json\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from ipywidgets import FloatProgress\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import re\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749a3855-5051-4fe1-b5a4-0c9c437e01da",
   "metadata": {},
   "source": [
    "### Transform the original slots into tokenized slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367ec8b0-36dc-4db5-a032-3680c631b490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_tokens(query, slots, tokens):\n",
    "    tokens = tokens[1:-1]\n",
    "    tagged_tokens = []\n",
    "    query_chars = list(query)\n",
    "    token_index = 0\n",
    "    index = 0\n",
    "    for char, slot in zip(query_chars, slots):\n",
    "        if token_index < len(tokens):\n",
    "            current_token = tokens[token_index]\n",
    "            if char == current_token or char in current_token:\n",
    "                tagged_tokens.append(slot)\n",
    "                token_index += 1\n",
    "            elif current_token.startswith('##'):\n",
    "                tagged_tokens.append(tagged_tokens[-1])\n",
    "                token_index += 1\n",
    "            elif '[UNK]' == current_token:\n",
    "                tagged_tokens.append('U')\n",
    "                token_index += 1\n",
    "    for i, item in enumerate(tagged_tokens):\n",
    "        if item == 'U':\n",
    "            if 'city' in tagged_tokens[i-1] and 'I-district' == tagged_tokens[i+1]:\n",
    "                tagged_tokens[i] = 'B-district'\n",
    "            if 'O' == tagged_tokens[i-1] and 'I-development' == tagged_tokens[i+1]:\n",
    "                tagged_tokens[i] = 'B-development'\n",
    "            if 'O' == tagged_tokens[i+1] and 'development' in tagged_tokens[i-1]:\n",
    "                tagged_tokens[i] = 'I-development'\n",
    "            if 'development' in tagged_tokens[i-1] and 'development' in tagged_tokens[i+1]:\n",
    "                tagged_tokens[i] = 'I-development'\n",
    "            if 'company' in tagged_tokens[i-1] and 'company' in tagged_tokens[i+1]:\n",
    "                tagged_tokens[i] = 'I-company'\n",
    "            if 'O' in tagged_tokens[i-1] and 'I-development' == tagged_tokens[i+1]:\n",
    "                tagged_tokens[i] = 'B-development'\n",
    "            if 'I-district' in tagged_tokens[i-1] and 'I-development' == tagged_tokens[i+1]:\n",
    "                tagged_tokens[i] = 'B-development'\n",
    "            else:\n",
    "                tagged_tokens[i] = 'O'\n",
    "    tagged_tokens.insert(0, 'O')\n",
    "    tagged_tokens.append('O')\n",
    "    return tagged_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7cd889-97b6-475f-89ea-44e0a584b43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_PATH = './model/bert-base-chinese'\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d26f24f-0cd3-4aac-b883-b9a2cbc518a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_512(input_string, max_pad_lenth=512):\n",
    "    while len(input_string) < max_pad_lenth:\n",
    "        input_string.append(int(-100))\n",
    "    return input_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c736d2-6069-4c38-8640-88b2e36319ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "intents_num = {'企业性质查询': 0,\n",
    "          '营业利润查询': 1,\n",
    "          '企业负债查询': 2,\n",
    "          '项目成交状况查询': 3,\n",
    "          '企业营业成本查询': 4,\n",
    "          '小区绿化率查询': 5,\n",
    "          '建筑密度查询': 6,\n",
    "          '小区成交均价查询': 7,\n",
    "          '营业总收入查询': 8,\n",
    "          '地块总价查询': 9,\n",
    "          '地块成交时间查询': 10,\n",
    "          '企业债务违约查询': 11,\n",
    "          '容积率查询': 12,\n",
    "          '企业风险查询': 13,\n",
    "          '地块归属查询': 14,\n",
    "          '项目开发商信息查询': 15\n",
    "          }\n",
    "\n",
    "slots_num = {'O': 0,\n",
    "          'B-year': 1,\n",
    "          'I-year': 2,\n",
    "          'B-month': 3,\n",
    "          'I-month': 4,\n",
    "          'B-city': 5,\n",
    "          'I-city': 6,\n",
    "          'B-district': 7,\n",
    "          'I-district': 8,\n",
    "          'B-development': 9,\n",
    "          'I-development': 10,\n",
    "          'B-company': 11,\n",
    "          'I-company': 12\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5b81aa-814f-467f-ad0d-64da4b8a9652",
   "metadata": {},
   "source": [
    "### Read the training, testing, and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac5721e-dba2-450e-b4ed-d1e0bdecc827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json2dataframe(all_datas):\n",
    "    df = pd.DataFrame(columns=['category', 'text', 'intents', 'slots'])\n",
    "    intents = []\n",
    "    for data in all_datas:\n",
    "        query = data['query']\n",
    "        slots = data['slots'].split(',')\n",
    "        encoded_text = tokenizer(query, return_tensors='pt')\n",
    "        tokens = tokenizer.convert_ids_to_tokens(encoded_text['input_ids'][0])\n",
    "        slots = tag_tokens(query, slots, tokens)\n",
    "        numbered_slots = [slots_num[item] for item in slots]\n",
    "        intents_label = [0.0] * len(intents_num)\n",
    "        if data['intent'] not in intents and '+' not in data['intent']:\n",
    "            intents.append(data['intent'])\n",
    "        if '+' in data['intent']:\n",
    "            data['intent'] = data['intent'].split('+')\n",
    "            for intent in data['intent']:\n",
    "                intents_label[intents_num[intent]] = 1.0\n",
    "            df = pd.concat([df, pd.DataFrame([{'category': intents_label, 'text': data['query'], 'intents': 1, 'slots': numbered_slots}])], ignore_index=True)\n",
    "        elif '+' not in data['intent']:\n",
    "            intent = data['intent']\n",
    "            intents_label[intents_num[intent]] = 1.0\n",
    "            df = pd.concat([df, pd.DataFrame([{'category': intents_label, 'text': data['query'], 'intents': 0, 'slots': numbered_slots}])], ignore_index=True)\n",
    "    df['slots'] = df['slots'].apply(pad_to_512)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30af4e9f-bab7-4826-aa05-805cc5411e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../dataset/train.json'\n",
    "with open(train_path, 'r', encoding='utf-8') as f:\n",
    "    all_datas = json.load(f)\n",
    "df_train = json2dataframe(all_datas)\n",
    "\n",
    "validation_path = '../dataset/validation.json'\n",
    "with open(validation_path, 'r', encoding='utf-8') as f:\n",
    "    all_datas = json.load(f)\n",
    "df_val = json2dataframe(all_datas)\n",
    "\n",
    "test_path = '../dataset/test.json'\n",
    "with open(test_path, 'r', encoding='utf-8') as f:\n",
    "    all_datas = json.load(f)\n",
    "df_test = json2dataframe(all_datas)\n",
    "\n",
    "print(len(df_train), len(df_val), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7588db-9a2e-417b-9118-7561a041189a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.labels = df['category']\n",
    "        self.texts = [tokenizer(text, \n",
    "                                padding='max_length', \n",
    "                                max_length = 512, \n",
    "                                truncation=True,\n",
    "                                return_tensors=\"pt\") \n",
    "                      for text in df['text']]\n",
    "        self.num_intents = df['intents']\n",
    "        self.slots = df['slots']\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def get_batch_num_intents(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return np.array(self.num_intents[idx])\n",
    "\n",
    "    def get_batch_slots(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return np.array(self.slots[idx])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "        batch_num = self.get_batch_num_intents(idx)\n",
    "        batch_slots = self.get_batch_slots(idx)\n",
    "        return batch_texts, batch_y, batch_num, batch_slots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91755315-4a01-4e1e-8ded-f2b90c62a815",
   "metadata": {},
   "source": [
    "### Build the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46526e65-4cbd-411a-8f6c-96fa101c7394",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.5):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-chinese')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear1 = nn.Linear(768, 16)\n",
    "        self.linear2 = nn.Linear(768, 2)\n",
    "        self.linear3 = nn.Linear(768, 13)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "        last_hidden_state, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear1_output = self.linear1(dropout_output)\n",
    "        intent_probability = self.sigmoid(linear1_output)\n",
    "        num_intents = self.linear2(dropout_output)\n",
    "        last_hidden_state_output = self.dropout(last_hidden_state)\n",
    "        slot_probability = self.linear3(last_hidden_state_output)\n",
    "        return intent_probability, num_intents, slot_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7516aed5-1d9f-491c-b5ae-0b7ca33088c5",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214047e4-7c0f-4fe5-99f4-b18843579fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensors_equal_ignore_order(tensor1, tensor2):\n",
    "    # Sort the two tensors along the specified dimension\n",
    "    sorted_tensor1, _ = torch.sort(tensor1)\n",
    "    sorted_tensor2, _ = torch.sort(tensor2)\n",
    "    results = []\n",
    "    for row1, row2 in zip(sorted_tensor1, sorted_tensor2):\n",
    "        results.append(torch.equal(row1, row2))\n",
    "        results_tensor = torch.tensor(results, dtype=torch.bool)\n",
    "    return results_tensor\n",
    "\n",
    "# Input two tensors: the first tensor is the probability tensor, and the second tensor is the one-hot encoded label tensor.\n",
    "def compute_multi_label_acc(probility, label):\n",
    "    probility, idx1 = torch.sort(probility, descending=True)\n",
    "    label, idx2 = torch.sort(label, descending=True)\n",
    "    idx1 = idx1[:,0:2]\n",
    "    idx2 = idx2[:,0:2]\n",
    "    for i,labl in enumerate(label):\n",
    "        if labl.sum() < 2:\n",
    "            idx1[i,1] = 0\n",
    "            idx2[i,1] = 0\n",
    "    acc = tensors_equal_ignore_order(idx1, idx2).sum().item()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bae768a-887b-4a57-9cbc-ba85366f6047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, val_data, learning_rate, epochs):\n",
    "    # Sort the two tensors along the specified dimension using the Dataset class to retrieve the training and validation sets\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "    # Use DataLoader to retrieve data based on batch_size, and shuffle samples during training\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=5, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=5)\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    # loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    binary_criterion = nn.BCELoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "    for epoch_num in range(epochs):\n",
    "        total_intent_acc_train = 0\n",
    "        total_num_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "        total_slot_acc_train = 0\n",
    "        total_tokens_train = 0\n",
    "        for train_input, train_intent_label, train_num_label, train_slot_label in tqdm(train_dataloader):\n",
    "            train_intent_label = train_intent_label.to(device)\n",
    "            train_num_label = train_num_label.to(device)\n",
    "            train_slot_label = train_slot_label.to(device)\n",
    "            mask = train_input['attention_mask'].squeeze(1).to(device)\n",
    "            input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "            # model output\n",
    "            intent_probability, num_intents, slot_probability = model(input_id, mask)\n",
    "            # compute loss\n",
    "            intent_loss = binary_criterion(intent_probability, train_intent_label.float())\n",
    "            active_loss = mask.view(-1) == 1\n",
    "            active_logits = slot_probability.view(-1, 13)[active_loss]\n",
    "            active_labels = train_slot_label.view(-1)[active_loss]\n",
    "            slot_loss = criterion(active_logits, active_labels)\n",
    "            num_loss = criterion(num_intents, train_num_label)\n",
    "            loss = intent_loss + num_loss + slot_loss\n",
    "            total_loss_train += loss.item()\n",
    "            # compute metric\n",
    "            intent_acc = compute_multi_label_acc(intent_probability, train_intent_label)\n",
    "            total_intent_acc_train += intent_acc\n",
    "            num_intent_acc = (num_intents.argmax(dim=1) == train_num_label).sum().item()\n",
    "            total_num_acc_train += num_intent_acc\n",
    "            word_leval_slots_acc = (slot_probability.argmax(dim=2).view(-1)[active_loss] == train_slot_label.view(-1)[active_loss]).sum().item()\n",
    "            batch_token_nums = active_loss.sum().item()\n",
    "            total_slot_acc_train += word_leval_slots_acc\n",
    "            total_tokens_train += batch_token_nums\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # ------- val -----------\n",
    "        total_intent_acc_val = 0\n",
    "        total_num_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "        total_slot_acc_val = 0\n",
    "        total_tokens_val = 0\n",
    "        with torch.no_grad():\n",
    "            for val_input, val_intent_label, val_num_label, val_slot_label in val_dataloader:\n",
    "                val_intent_label = val_intent_label.to(device)\n",
    "                val_num_label = val_num_label.to(device)\n",
    "                val_slot_label = val_slot_label.to(device)\n",
    "                mask = val_input['attention_mask'].squeeze(1).to(device)\n",
    "                input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "                intent_probability, num_intents, slot_probability = model(input_id, mask)\n",
    "                intent_loss = binary_criterion(intent_probability, val_intent_label.float())\n",
    "                # compute slots loss\n",
    "                active_loss = mask.view(-1) == 1\n",
    "                active_logits = slot_probability.view(-1, 13)[active_loss]\n",
    "                active_labels = val_slot_label.view(-1)[active_loss]\n",
    "                slot_loss = criterion(active_logits, active_labels)\n",
    "                # compute intent num loss\n",
    "                num_loss = criterion(num_intents, val_num_label)\n",
    "                loss = intent_loss + num_loss + slot_loss\n",
    "                total_loss_val += loss.item()\n",
    "                # compute metric\n",
    "                intent_acc = compute_multi_label_acc(intent_probability, val_intent_label)\n",
    "                total_intent_acc_val += intent_acc\n",
    "                num_intent_acc = (num_intents.argmax(dim=1) == val_num_label).sum().item()\n",
    "                total_num_acc_val += num_intent_acc\n",
    "                word_leval_slots_acc = (slot_probability.argmax(dim=2).view(-1)[active_loss] == val_slot_label.view(-1)[active_loss]).sum().item()\n",
    "                batch_token_nums = active_loss.sum().item()\n",
    "                total_slot_acc_val += word_leval_slots_acc\n",
    "                total_tokens_val += batch_token_nums\n",
    "        writer.add_scalar('Loss/train', total_loss_train / len(train_data), epoch_num)\n",
    "        writer.add_scalar('Accuracy/train_intent', total_intent_acc_train / len(train_data), epoch_num)\n",
    "        writer.add_scalar('Accuracy/train_num_intents', total_num_acc_train / len(train_data), epoch_num)\n",
    "        writer.add_scalar('Accuracy/train_token_level_slot_acc', total_slot_acc_train / total_tokens_train, epoch_num)\n",
    "        writer.add_scalar('Loss/val', total_loss_val / len(val_data), epoch_num)\n",
    "        writer.add_scalar('Accuracy/val_intent', total_intent_acc_val / len(val_data), epoch_num)\n",
    "        writer.add_scalar('Accuracy/val_num_intents', total_num_acc_val / len(val_data), epoch_num)\n",
    "        writer.add_scalar('Accuracy/val_token_level_slot_acc', total_slot_acc_val / total_tokens_val, epoch_num)\n",
    "\n",
    "        print(\n",
    "            f'''Epochs: {epoch_num + 1} \n",
    "            | Train Loss: {total_loss_train / len(train_data): .3f} \n",
    "            | Train Intent Accuracy: {total_intent_acc_train / len(train_data): .3f}\n",
    "            | Train Num of intents Accuracy: {total_num_acc_train / len(train_data): .3f} \n",
    "            | Train Token-level Slots Accuracy: {total_slot_acc_train / total_tokens_train: .3f} \n",
    "            | Val Loss: {total_loss_val / len(val_data): .3f} \n",
    "            | Val Intent Accuracy: {total_intent_acc_val / len(val_data): .3f}\n",
    "            | Val Num of intents Accuracy: {total_num_acc_val / len(val_data): .3f}\n",
    "            | Val Token-level Slots Accuracy: {total_slot_acc_val / total_tokens_val: .3f} ''')\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecbd013-4e9e-4d3c-85d3-66384ecde676",
   "metadata": {},
   "source": [
    "### Begin train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4b40c0-48a3-4b36-8fbb-2423baa6a49f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "writer = SummaryWriter('./runs')\n",
    "model = BertClassifier()\n",
    "LR = 1e-6\n",
    "train(model, df_train, df_val, LR, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc238d74-3d14-46c7-bcda-259934b10775",
   "metadata": {},
   "source": [
    "### validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92404b39-b623-4616-b0e3-eba3dc6885ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_data):\n",
    "    test = Dataset(test_data)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=4)\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    total_intent_acc_test = 0\n",
    "    total_num_acc_test = 0\n",
    "    total_tokens_test = 0\n",
    "    total_slot_acc_test = 0\n",
    "    with torch.no_grad():\n",
    "        for test_input, test_intent_label, test_num_label, test_slot_label in test_dataloader:\n",
    "            test_intent_label = test_intent_label.to(device)\n",
    "            test_num_label = test_num_label.to(device)\n",
    "            test_slot_label =  test_slot_label.to(device)\n",
    "            mask = test_input['attention_mask'].squeeze(1).to(device)\n",
    "            input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "            intent_probability, num_intents, slot_probability = model(input_id, mask)\n",
    "            active_loss = mask.view(-1) == 1\n",
    "            intent_acc = compute_multi_label_acc(intent_probability, test_intent_label)\n",
    "            # intent_acc = (intent_probability.argmax(dim=1) == test_intent_label.argmax(dim=1)).sum().item()\n",
    "            total_intent_acc_test += intent_acc\n",
    "            num_intent_acc = (num_intents.argmax(dim=1) == test_num_label).sum().item()\n",
    "            total_num_acc_test += num_intent_acc\n",
    "            word_leval_slots_acc = (slot_probability.argmax(dim=2).view(-1)[active_loss] == test_slot_label.view(-1)[active_loss]).sum().item()\n",
    "            batch_token_nums = active_loss.sum().item()\n",
    "            total_slot_acc_test += word_leval_slots_acc\n",
    "            total_tokens_test += batch_token_nums\n",
    "    print(f'Test Intent Accuracy: {total_intent_acc_test*100 / len(test_data): .2f}%')\n",
    "    print(f'Test Num of Intent Accuracy: {total_num_acc_test*100 / len(test_data): .2f}%')\n",
    "    print(f'Test Token-level Slots Accuracy: {total_slot_acc_test*100 / total_tokens_test: .2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cf5998-3300-45e1-8ff8-4c1104f9e282",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bc7775-0802-44c9-9a79-f804f1eda576",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './model/bert.pt'\n",
    "torch.save(model, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d45d62-cd97-4453-9508-ac39b2227c6a",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d3f77a-0b14-49f9-bdb0-02b41b20bf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top2_indices(tensor):\n",
    "    if len(tensor) < 2:\n",
    "        raise ValueError(\"Input tensor must have at least 2 elements\")\n",
    "    _, indices = torch.topk(tensor, k=2, dim=0)\n",
    "    return indices\n",
    "\n",
    "def find_key(dictionary, value):\n",
    "    return [key for key, val in dictionary.items() if val == value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f72d53d-922d-44b3-8b6c-4fc34bf13d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../model/bert.pt'\n",
    "model = torch.load(model_path)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "BERT_PATH = '../model/bert-base-chinese'\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db343b7c-3638-446b-8f98-08972ce73bd8",
   "metadata": {},
   "source": [
    "### Transform slots into keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8660983-7a3a-42d6-b3d2-a7cb43928173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_tokens_with_query(tokens, query, query_slot):\n",
    "    query = list(query)\n",
    "    new_tokens = []\n",
    "    origin_slot = query_slot\n",
    "    if isinstance(origin_slot, type('str')):\n",
    "        origin_slot = origin_slot.split(' ')\n",
    "    new_slot = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token == '[CLS]' or token == '[SEP]':\n",
    "            continue\n",
    "        elif token == query[0]:\n",
    "            new_tokens.append(token)\n",
    "            new_slot.append(origin_slot[0])\n",
    "            query = query[1:]\n",
    "            origin_slot = origin_slot[1:]\n",
    "        elif '##' in token:\n",
    "            token = token[2:]\n",
    "            new_tokens.append(token)\n",
    "            new_slot.append(origin_slot[0])\n",
    "            for t in list(token):\n",
    "                if t == query[0]:\n",
    "                    query = query[1:]\n",
    "                    origin_slot = origin_slot[1:]\n",
    "                    \n",
    "        elif '[UNK]' == token:\n",
    "            end_index = query.index(tokens[i+1])\n",
    "            unk = ''.join(query[0:end_index])\n",
    "            new_tokens.append(unk)\n",
    "            new_slot.append(origin_slot[0])\n",
    "            query = query[end_index:]\n",
    "            origin_slot = origin_slot[1:]\n",
    "\n",
    "        elif len(token)>1:\n",
    "            new_tokens.append(token)\n",
    "            new_slot.append(origin_slot[0])\n",
    "            for t in list(token):\n",
    "                if t == query[0]:\n",
    "                    query = query[1:]\n",
    "                    origin_slot = origin_slot[1:]\n",
    "    return new_tokens, new_slot\n",
    "\n",
    "def restore_keywords_from_tokens(tokens, token_slot):\n",
    "    keywords = []\n",
    "    current_tokens = []\n",
    "    current_label = None\n",
    "    token_slot = token_slot[1:-1]\n",
    "\n",
    "    for token, slot in zip(tokens, token_slot):\n",
    "        if slot.startswith('B-'):\n",
    "            if current_tokens:\n",
    "                keywords.append((''.join(current_tokens), current_label))\n",
    "                current_tokens = []\n",
    "            current_label = slot[2:]\n",
    "            current_tokens.append(token)\n",
    "        elif slot.startswith('I-') and current_label == slot[2:]:\n",
    "            current_tokens.append(token)\n",
    "        else:\n",
    "            if current_tokens:\n",
    "                keywords.append((''.join(current_tokens), current_label))\n",
    "                current_tokens = []\n",
    "                current_label = None\n",
    "\n",
    "    if current_tokens:\n",
    "        keywords.append((''.join(current_tokens), current_label))\n",
    "\n",
    "    return keywords\n",
    "\n",
    "def restore_keywords_from_query(query, slots):\n",
    "    keywords = []\n",
    "    current_tokens = []\n",
    "    current_label = None\n",
    "    query = list(query)\n",
    "    if slots[0] == '[CLS]':\n",
    "        slots = slots[1:-1]\n",
    "\n",
    "    for token, slot in zip(query, slots):\n",
    "        if slot.startswith('B-'):\n",
    "            if current_tokens:\n",
    "                keywords.append((''.join(current_tokens), current_label))\n",
    "                current_tokens = []\n",
    "            current_label = slot[2:]\n",
    "            current_tokens.append(token)\n",
    "        elif slot.startswith('I-') and current_label == slot[2:]:\n",
    "            current_tokens.append(token)\n",
    "        else:\n",
    "            if current_tokens:\n",
    "                keywords.append((''.join(current_tokens), current_label))\n",
    "                current_tokens = []\n",
    "                current_label = None\n",
    "\n",
    "    if current_tokens:\n",
    "        keywords.append((''.join(current_tokens), current_label))\n",
    "\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19204bf0-381a-4410-951d-940ddfd65e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../model/bert.pt'\n",
    "model = torch.load(model_path)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "BERT_PATH = '../model/bert-base-chinese'\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4793fb53-ba10-4b97-98dd-bbf722ceeec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intent2label(intents_row):\n",
    "    intents_num = {'企业性质查询': 0,\n",
    "          '营业利润查询': 1,\n",
    "          '企业负债查询': 2,\n",
    "          '项目成交状况查询': 3,\n",
    "          '企业营业成本查询': 4,\n",
    "          '小区绿化率查询': 5,\n",
    "          '建筑密度查询': 6,\n",
    "          '小区成交均价查询': 7,\n",
    "          '营业总收入查询': 8,\n",
    "          '地块总价查询': 9,\n",
    "          '地块成交时间查询': 10,\n",
    "          '企业债务违约查询': 11,\n",
    "          '容积率查询': 12,\n",
    "          '企业风险查询': 13,\n",
    "          '地块归属查询': 14,\n",
    "          '项目开发商信息查询': 15\n",
    "          }\n",
    "    intents_label = [0] * len(intents_num)\n",
    "    if '+' in intents_row:\n",
    "        intents = intents_row.split('+')\n",
    "        for intent in intents:\n",
    "            intents_label[intents_num[intent]] = 1\n",
    "    elif '+' not in intents_row:\n",
    "        intent = intents_row\n",
    "        intents_label[intents_num[intent]] = 1\n",
    "    return intents_label\n",
    "\n",
    "def extract_keywords(tokens, token_slot):\n",
    "    keywords = []\n",
    "    current_keyword = ''\n",
    "    current_slot = ''\n",
    "    \n",
    "    for token, slot in zip(tokens[1:-1], token_slot[1:-1]):  # Skip [CLS] and [SEP]\n",
    "        if slot.startswith('B-'):\n",
    "            if current_keyword:\n",
    "                keywords.append((current_keyword.strip(), current_slot.split('-')[1]))\n",
    "            current_keyword = token\n",
    "            current_slot = slot\n",
    "        elif slot.startswith('I-') and slot[2:] == current_slot[2:]:\n",
    "            current_keyword += token\n",
    "        elif slot == 'O':\n",
    "            if current_keyword:\n",
    "                keywords.append((current_keyword.strip(), current_slot.split('-')[1]))\n",
    "                current_keyword = ''\n",
    "                current_slot = ''\n",
    "    \n",
    "    if current_keyword:\n",
    "        keywords.append((current_keyword.strip(), current_slot.split('-')[1]))\n",
    "    \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2973507-cfa1-4f51-b80d-ae75e05618d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = '../dataset/test.json'\n",
    "with open(data_file, 'r', encoding='utf-8') as f:\n",
    "    datas = json.load(f)\n",
    "pred_intent_num = []\n",
    "true_intent_num = []\n",
    "pred_intent_label = []\n",
    "true_intent_label = []\n",
    "true_key_words = []\n",
    "pred_key_words = []\n",
    "true_token_slots = []\n",
    "pred_token_slots = []\n",
    "i = 0\n",
    "for data in tqdm(datas):\n",
    "    query = data['query']\n",
    "    #print(query)\n",
    "    origin_slots = data['slots'].split(',')\n",
    "    #print(origin_slots)\n",
    "    true_key_word = restore_keywords_from_query(query, origin_slots)\n",
    "    #print(f'true_key_word:{true_key_word}')\n",
    "    true_key_words.append(true_key_word)\n",
    "    intent = data['intent']\n",
    "    if '+' in intent:\n",
    "        true_intent_num.append(1)\n",
    "    if '+' not in intent:\n",
    "        true_intent_num.append(0)\n",
    "    intent_label = intent2label(intent)\n",
    "    true_intent_label.append(list(intent_label))\n",
    "    encoded_text = tokenizer(query, return_tensors='pt')\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoded_text['input_ids'][0])\n",
    "    new_tokens, new_slots = align_tokens_with_query(tokens, query, origin_slots)\n",
    "    #print(f'new_tokens:{new_tokens}')\n",
    "    new_slots_to_num = [slots_num[item] for item in new_slots]\n",
    "    true_token_slots.extend(new_slots_to_num)\n",
    "    encoded_text_id = encoded_text['input_ids'].to(device)\n",
    "    mask = encoded_text['attention_mask'].to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(encoded_text_id, mask)\n",
    "    intent_probility = outputs[0].view(-1)\n",
    "    _, intent_idx = torch.topk(intent_probility, k=2, dim=0)\n",
    "    intent_idx = intent_idx.cpu()\n",
    "    intent_num_probility = outputs[1].argmax()\n",
    "    pred_intent_num.append(intent_num_probility.cpu())\n",
    "    if intent_num_probility == 0:\n",
    "        intent_idx = intent_idx[0]\n",
    "    pred_intent = [1 if i in intent_idx else 0 for i in range(16)]\n",
    "    pred_intent_label.append(list(pred_intent))\n",
    "    slots_probility = outputs[2].argmax(dim=2).view(-1)\n",
    "    pred_slots_num = slots_probility[1:-1].cpu()\n",
    "    pred_token_slots.extend([t.item() for t in pred_slots_num])\n",
    "    pred_token_slot = [find_key(slots_num, i)[0] for i in slots_probility]\n",
    "    #print(f'token_slot:{token_slot}')\n",
    "    pred_key_word = restore_keywords_from_tokens(new_tokens, pred_token_slot)\n",
    "    #print(f'pred_key_word:{pred_key_word}')\n",
    "    pred_key_words.append(pred_key_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c26ed0-8aa8-48a0-be6e-ef2af037663b",
   "metadata": {},
   "source": [
    "# Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704974b1-2962-4697-9e15-ec89d392df56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_compute(trues: list, preds: list):\n",
    "    if len(trues) != len(preds):\n",
    "        return 'Input lengthes not equal!'\n",
    "    precision = 0\n",
    "    precision_all = 0\n",
    "    recall = 0\n",
    "    recall_all = 0\n",
    "    for true_label, pred_label in zip(trues, preds):\n",
    "        # 查准率\n",
    "        for pred in pred_label:\n",
    "            if pred in true_label:\n",
    "                precision += 1\n",
    "            precision_all\n",
    "        # 查全率\n",
    "        for true in true_label:\n",
    "            if true in pred_label:\n",
    "                recall += 1\n",
    "            recall_all += 1\n",
    "    P = precision/precision_all\n",
    "    R = recall/recall_all\n",
    "    F1 = 2 * P * R / (P + R)\n",
    "    return P, R, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49686a88-d89a-47d6-afe3-7f9ea1801904",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_intent_num = pred_intent_num.cpu()\n",
    "target_names = ['One Intent', 'Two Intent']\n",
    "print(classification_report(true_intent_num , pred_intent_num, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b697755-c3e9-48b3-898f-fad277137d54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "city_count = 0\n",
    "city_p = 0\n",
    "development_count = 0\n",
    "development_p = 0\n",
    "company_count = 0\n",
    "company_p = 0\n",
    "community_count = 0\n",
    "community_p = 0\n",
    "year_count = 0\n",
    "year_p = 0\n",
    "month_count = 0\n",
    "month_p = 0\n",
    "acc_count = 0\n",
    "for true_label, pred_label in zip(true_key_words, pred_key_words):\n",
    "    true_year_value = []\n",
    "    true_month_value = []\n",
    "    true_community_value = []\n",
    "    true_city_value = []\n",
    "    true_company_value = []\n",
    "    true_development_value = []\n",
    "    for key_word, slot in true_label:\n",
    "        #print(slot)\n",
    "        if slot == 'year':\n",
    "            true_year_value.append(key_word)\n",
    "            pred_year_value = [key[0] for key in pred_label if key[1] == 'year']\n",
    "            \n",
    "        elif slot == 'month':\n",
    "            true_month_value.append(key_word)\n",
    "            pred_month_value = [key[0] for key in pred_label if key[1] == 'month']\n",
    "            \n",
    "        elif slot == 'district':\n",
    "            true_community_value.append(key_word)\n",
    "            # print(true_community_value)\n",
    "            pred_community_value = [key[0] for key in pred_label if key[1] == 'district']\n",
    "            \n",
    "        elif slot == 'city':\n",
    "            true_city_value.append(key_word)\n",
    "            pred_city_value = [key[0] for key in pred_label if key[1] == 'city']\n",
    "            \n",
    "        elif slot == 'development':\n",
    "            true_development_value.append(key_word)\n",
    "            pred_development_value = [key[0] for key in pred_label if key[1] == 'development']\n",
    "            \n",
    "        elif slot == 'company':\n",
    "            true_company_value.append(key_word)\n",
    "            pred_company_value = [key[0] for key in pred_label if key[1] == 'company']\n",
    "\n",
    "    if true_city_value != []:\n",
    "        if set(true_city_value) == set(pred_city_value):\n",
    "            city_p += 1\n",
    "        city_count += 1\n",
    "    if true_year_value != []:\n",
    "        if set(true_year_value) == set(pred_year_value):\n",
    "            year_p += 1\n",
    "        year_count += 1\n",
    "\n",
    "    if true_month_value != []:\n",
    "        if set(true_month_value) == set(pred_month_value):\n",
    "            month_p += 1\n",
    "        month_count += 1\n",
    "        \n",
    "    if true_community_value != []:\n",
    "        if set(true_community_value) == set(pred_community_value):\n",
    "            community_p += 1\n",
    "        community_count += 1\n",
    "\n",
    "    if true_company_value != []:\n",
    "        if set(true_company_value) == set(pred_company_value):\n",
    "            company_p += 1\n",
    "        company_count += 1\n",
    "\n",
    "    if true_development_value != []:\n",
    "        if set(true_development_value) == set(pred_development_value):\n",
    "            development_p += 1\n",
    "        development_count += 1\n",
    "\n",
    "    if true_city_value != []:\n",
    "        if set(true_city_value) == set(pred_city_value):\n",
    "            city_p += 1\n",
    "        city_count += 1\n",
    "    if set(true_label) == set(pred_label):\n",
    "        acc_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfb68a5-59b1-4673-94e8-1eb8fd6be9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = {'city': 0, 'development': 0, 'company': 0, 'district': 0, 'year': 0, 'month': 0, 'all': 0 }\n",
    "precision_all = {'city': 0, 'development': 0, 'company': 0, 'district': 0, 'year': 0, 'month': 0, 'all': 0 }\n",
    "recall = {'city': 0, 'development': 0, 'company': 0, 'district': 0, 'year': 0, 'month': 0, 'all': 0 }\n",
    "recall_all = {'city': 0, 'development': 0, 'company': 0, 'district': 0, 'year': 0, 'month': 0, 'all': 0 }\n",
    "\n",
    "for true_label, pred_label in zip(true_key_words, pred_key_words):\n",
    "    for pred in pred_label:\n",
    "        slot = pred[1]\n",
    "        keyword = pred[0]\n",
    "        if pred in true_label:\n",
    "            precision['all'] += 1\n",
    "            precision[slot] += 1\n",
    "        precision_all['all'] += 1\n",
    "        precision_all[slot] += 1\n",
    "    for true in true_label:\n",
    "        slot = true[1]\n",
    "        keyword = true[0]\n",
    "        if true in pred_label:\n",
    "            recall['all'] += 1\n",
    "            recall[slot] += 1\n",
    "        recall_all['all'] += 1\n",
    "        recall_all[slot] += 1\n",
    "Macro = {'P': 0, 'R': 0, 'F1': 0}\n",
    "Micro = {'P': 0, 'R': 0, 'F1': 0}\n",
    "for key in precision.keys():\n",
    "    P = precision[key]/precision_all[key]\n",
    "    if key != 'all':\n",
    "        Macro['P'] += P\n",
    "        Micro['P'] += P\n",
    "    R = recall[key]/recall_all[key]\n",
    "    if key != 'all':\n",
    "        Macro['R'] += R\n",
    "        Micro['R'] += R\n",
    "    F1 = 2*P*R/(P+R)\n",
    "Macro_P = Macro['P']/6\n",
    "Macro_R = Macro['R']/6\n",
    "Macro_F1 = 2*Macro_P*Macro_R/(Macro_P+Macro_R)\n",
    "Micro_P = Micro['P']/6\n",
    "Micro_R = Micro['R']/6\n",
    "Micro_F1 = 2*Micro_P*Micro_R/(Micro_P+Micro_R)\n",
    "print(f'Macro_P:{Macro_P}')\n",
    "print(f'Macro_R:{Macro_R}')\n",
    "print(f'Macro_F1:{Macro_F1}')\n",
    "print(f'Micro_P:{Micro_P}')\n",
    "print(f'Micro_R:{Micro_R}')\n",
    "print(f'Micro_F1:{Micro_F1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ad6b78-6fe8-4dc2-bd14-d72a911c6fb9",
   "metadata": {},
   "source": [
    "### Save BERT's predicted SLU information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01501a16-96bf-4cee-9b15-eb5c04aaa488",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_key_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18bb457-edee-47c8-a576-ab0ee6a9920d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_tokens_with_query(tokens, query):\n",
    "    query = list(query)\n",
    "    new_tokens = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token == '[CLS]' or token == '[SEP]':\n",
    "            continue\n",
    "        elif token == query[0]:\n",
    "            new_tokens.append(token)\n",
    "            query = query[1:]\n",
    "        elif '##' in token:\n",
    "            token = token[2:]\n",
    "            new_tokens.append(token)\n",
    "            for t in list(token):\n",
    "                if t == query[0]:\n",
    "                    query = query[1:]\n",
    "                    \n",
    "        elif '[UNK]' == token:\n",
    "            end_index = query.index(tokens[i+1])\n",
    "            unk = ''.join(query[0:end_index])\n",
    "            new_tokens.append(unk)\n",
    "            query = query[end_index:]\n",
    "\n",
    "        elif len(token)>1:\n",
    "            new_tokens.append(token)\n",
    "            for t in list(token):\n",
    "                if t == query[0]:\n",
    "                    query = query[1:]\n",
    "    return new_tokens\n",
    "\n",
    "def restore_keywords_from_tokens(tokens, token_slot):\n",
    "    keywords = []\n",
    "    current_tokens = []\n",
    "    current_label = None\n",
    "    token_slot = token_slot[1:-1]\n",
    "\n",
    "    for token, slot in zip(tokens, token_slot):\n",
    "        if slot.startswith('B-'):\n",
    "            if current_tokens:\n",
    "                keywords.append((''.join(current_tokens), current_label))\n",
    "                current_tokens = []\n",
    "            current_label = slot[2:]\n",
    "            current_tokens.append(token)\n",
    "        elif slot.startswith('I-') and current_label == slot[2:]:\n",
    "            current_tokens.append(token)\n",
    "        else:\n",
    "            if current_tokens:\n",
    "                keywords.append((''.join(current_tokens), current_label))\n",
    "                current_tokens = []\n",
    "                current_label = None\n",
    "\n",
    "    if current_tokens:\n",
    "        keywords.append((''.join(current_tokens), current_label))\n",
    "    keyword_pair = []\n",
    "    for keyword in keywords:\n",
    "        if keyword[-1] == 'city':\n",
    "            keyword_pair.append(f'城市:{keyword[0]}')\n",
    "        elif keyword[-1] == 'district':\n",
    "            keyword_pair.append(f'区域:{keyword[0]}')\n",
    "        elif keyword[-1] == 'development':\n",
    "            keyword_pair.append(f'项目名称:{keyword[0]}')\n",
    "        elif keyword[-1] == 'company':\n",
    "            keyword_pair.append(f'企业名称:{keyword[0]}')\n",
    "        elif keyword[-1] == 'year':\n",
    "            keyword_pair.append(f'年份:{keyword[0]}')\n",
    "        elif keyword[-1] == 'month':\n",
    "            keyword_pair.append(f'月份:{keyword[0]}')\n",
    "        elif keyword[-1] == 'land':\n",
    "            keyword_pair.append(f'地块名称:{keyword[0]}')\n",
    "\n",
    "    return keyword_pair\n",
    "\n",
    "\n",
    "def restore_keywords_from_query(query, slots):\n",
    "    keywords = []\n",
    "    current_tokens = []\n",
    "    current_label = None\n",
    "    query = list(query)\n",
    "    if isinstance(slots, str):\n",
    "        slots = slots.split(' ')\n",
    "    if slots[0] == '[CLS]':\n",
    "        slots = slots[1:-1]\n",
    "\n",
    "    for token, slot in zip(query, slots):\n",
    "        if slot.startswith('B-'):\n",
    "            if current_tokens:\n",
    "                keywords.append((''.join(current_tokens), current_label))\n",
    "                current_tokens = []\n",
    "            current_label = slot[2:]\n",
    "            current_tokens.append(token)\n",
    "        elif slot.startswith('I-') and current_label == slot[2:]:\n",
    "            current_tokens.append(token)\n",
    "        else:\n",
    "            if current_tokens:\n",
    "                keywords.append((''.join(current_tokens), current_label))\n",
    "                current_tokens = []\n",
    "                current_label = None\n",
    "\n",
    "    if current_tokens:\n",
    "        keywords.append((''.join(current_tokens), current_label))\n",
    "    keyword_pair = []\n",
    "    for keyword in keywords:\n",
    "        if keyword[-1] == 'city':\n",
    "            keyword_pair.append(f'城市:{keyword[0]}')\n",
    "        elif keyword[-1] == 'district':\n",
    "            keyword_pair.append(f'区域:{keyword[0]}')\n",
    "        elif keyword[-1] == 'development':\n",
    "            keyword_pair.append(f'项目名称:{keyword[0]}')\n",
    "        elif keyword[-1] == 'company':\n",
    "            keyword_pair.append(f'企业名称:{keyword[0]}')\n",
    "        elif keyword[-1] == 'year':\n",
    "            keyword_pair.append(f'年份:{keyword[0]}')\n",
    "        elif keyword[-1] == 'month':\n",
    "            keyword_pair.append(f'月份:{keyword[0]}')\n",
    "        elif keyword[-1] == 'land':\n",
    "            keyword_pair.append(f'地块名称:{keyword[0]}')\n",
    "\n",
    "    return keyword_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede2355d-380b-47a6-bf0b-339391e9d722",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = '../dataset/test.json'\n",
    "with open(json_file, 'r', encoding='utf-8') as f:\n",
    "    datas = json.load(f)\n",
    "all_intents = ['小区绿化率查询', '营业利润查询', '小区成交均价查询+项目成交状况查询', '企业营业成本查询', '企业负债查询', '小区成交均价查询', '企业风险查询', '建筑密度查询', '地块总价查询+容积率查询', '容积率查询', '项目成交状况查询', '地块成交时间查询+地块总价查询', '地块成交时间查询', '企业债务违约查询', '营业总收入查询', '地块总价查询', '企业性质查询', '容积率查询+地块成交时间查询', '地块成交时间查询+容积率查询', '项目开发商信息查询', '小区绿化率查询+容积率查询', '地块归属查询', '企业债务违约查询+企业负债查询', '企业负债查询+企业风险查询']\n",
    "for data in datas:\n",
    "    query = data['query']\n",
    "    encoded_text = tokenizer(query, return_tensors='pt')\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoded_text['input_ids'][0])\n",
    "    new_tokens = align_tokens_with_query(tokens, query)\n",
    "    # print(f'new_tokens:{new_tokens}')\n",
    "    encoded_text_id = encoded_text['input_ids'].to(device)\n",
    "    mask = encoded_text['attention_mask'].to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(encoded_text_id, mask)\n",
    "    intent_probility = outputs[0].view(-1)\n",
    "    _, intent_idx = torch.topk(intent_probility, k=2, dim=0)\n",
    "    intent_idx = intent_idx.cpu()\n",
    "    intent_num = outputs[1].argmax().cpu()\n",
    "    if intent_num == 0:\n",
    "        intent_idx = intent_idx[0]\n",
    "        intent = find_key(intents_num, intent_idx)[0]\n",
    "    elif intent_num == 1:\n",
    "        # intent_idx = intent_idx.item()\n",
    "        intent = [find_key(intents_num, i)[0] for i in intent_idx]\n",
    "        if intent[0]+'+'+intent[1] in all_intents:\n",
    "            intent = intent[0]+'+'+intent[1]\n",
    "        if intent[1]+'+'+intent[0] in all_intents:\n",
    "            intent = intent[1]+'+'+intent[0]\n",
    "    data[\"BERT_pred_intent\"] = intent\n",
    "    slots_probility = outputs[2].argmax(dim=2).view(-1)\n",
    "    token_slot = [find_key(slots_num, i)[0] for i in slots_probility]\n",
    "    pred_key_word = restore_keywords_from_tokens(new_tokens, token_slot)\n",
    "    key_word = pred_key_word\n",
    "    data[\"BERT_pred_slots\"] = key_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c97be6-05c1-46cc-bd98-1d26f6fac8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json_file = './results/test-with-BERT_pred_intent+slots.json'\n",
    "with open(save_json_file, 'w', encoding='utf-8') as file:\n",
    "    json.dump(datas, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdb3a5d-c79e-4a97-b855-d9b075d0bf08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500e2bb6-de20-49f9-83e6-0cbc59e8c78b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
